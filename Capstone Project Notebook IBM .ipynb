Week 3
Task 1
Download and explore Toronto Dataset

After a websearch for how to extract data saved on a website using pandas, we download the information on the wikipedia page using the read_html method and look at the data:
`import pandas as pd
df_scraped = pd.read_html('https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_M', header = 0)
df_scraped`

`df_scraped[0]`

`df_toronto = df_scraped[0]
df_toronto`

`df_toronto_to_drop = (df_toronto['Borough'] == 'Not assigned')
df_toronto_not_to_drop = ~df_toronto_to_drop
df_toronto = df_toronto[df_toronto_not_to_drop]
print(df_toronto.shape)
df_toronto`


`#df_toronto.reset_index(inplace=True)`
`# Looking at the data frame shows that there are no Neighbourhoods with the 'Not assigned' entry
# However, we proceed with the case that there will be an entry with 'Not assigned in the future'

# we follow up on the same idea and assign the name of the borough to the neighborhood for each identified index
indices = []
df_toronto.iloc[:,:]

# We search for the indices where the neighborhood has a 'not assigned' attribute
series1 = df_toronto['Neighbourhood']
liste1 = series1.tolist()

series2 = df_toronto['Borough']
liste2 = series2.tolist()

for i in range(len(liste1)):
    if liste1[i] == 'Not assigned':
        print(liste1[i])
        indices.append(i)
        liste1[i] = liste2[i]
        print('change made in index ', i)
    else:
        print('ok, no change for index', i, 'as there is no "not assigned" entry for this index')
indices`


**We are now done with part 1 of the assignment, our final data frame and its shape are:**

`df_toronto`

`df_toronto.shape`


**Task 2**

I get an error message when I try to import the geocoder, thus I use the link to the csv file to import it:

`path = 'https://cocl.us/Geospatial_data'
df_geospatial = pd.read_csv(path)
df_geospatial`

**We need to correct the index of the df_toronto data frame in order to merge the two frames later on.**

`df_toronto.reset_index(inplace=True)`

`#df_toronto.rename(columns={'index' : 'index', 'Postal Code' : 'Postal Code', 'Borough' : 'Borough', 'Neighbourhood' : 'Neighborhood'})`

**We note that the dataframe of the geospatial data begins with different Postal Codes than the df_toronto dataframe starts with. Thus, we first sort both of the data frames such that there order is the same**

`df_geospatial.sort_values('Postal Code', axis=0, inplace=True)
df_geospatial`

**We repeat the same sorting operation with the df_toronto dataframe**

`df_toronto.sort_values('Postal Code', axis=0, inplace=True)
df_toronto`

`

/opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages/ipykernel/__main__.py:1: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  if __name__ == '__main__':

`

`df_toronto_merged = df_toronto
df_toronto_merged['Latitude'] = df_geospatial['Latitude']
df_toronto_merged['Longitude'] = df_geospatial['Longitude']`

`/opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages/ipykernel/__main__.py:2: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  from ipykernel import kernelapp as app
/opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages/ipykernel/__main__.py:3: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  app.launch_new_instance()`

`df_toronto_merged`

`df_toronto.reset_index(inplace=True)
df_toronto`

We get rid of columns we do not need.

`df_toronto_merged.drop(['index','level_0'], axis=1, inplace=True)
df_toronto_merged`

`/opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages/pandas/core/frame.py:3997: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  errors=errors,`


Our following dataframe is sorted by the postal code and is the result of merging the geospatial data (sorted by postal code) and the toronto dataframe (also sorted by postal code):

`df_toronto_merged.sort_values('Postal Code', axis=0,inplace=True)`

`/opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages/ipykernel/__main__.py:1: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  if __name__ == '__main__':`

`df_toronto_merged.head(50)`


**Task 3**

We take the code from the New York Neighborhood Lab in order to import all necessary libraries:

`# library to handle JSON files

!conda install -c conda-forge geopy --yes # uncomment this line if you haven't completed the Foursquare API lab
from geopy.geocoders import Nominatim # convert an address into latitude and longitude values

import requests # library to handle requests
from pandas.io.json import json_normalize # tranform JSON file into a pandas dataframe

# Matplotlib and associated plotting modules
import matplotlib.cm as cm
import matplotlib.colors as colors

# import k-means from clustering stage
from sklearn.cluster import KMeans

!conda install -c conda-forge folium=0.5.0 --yes # uncomment this line if you haven't completed the Foursquare API lab
import folium # map rendering library

print('Libraries imported.')`

`!pip install folium as folium`

We get the longitude and latitude for Toronto either via a google search or via the command below where we use the address 'Toronto,ON,Canada' which we also obtained via a google search:

`address = 'Toronto, ON, Canada' 

geolocator = Nominatim(user_agent="tor_explorer")
location = geolocator.geocode(address)
latitude = location.latitude
longitude = location.longitude
print('The geograpical coordinates of Toronto are {}, {}.'.format(latitude, longitude))`

We create a map of toronto

`import folium
map_toronto = folium.Map(location=[latitude, longitude], zoom_start=10)

for lat, lng, borough, neighborhood in zip(df_toronto_merged['Latitude'], df_toronto_merged['Longitude'], df_toronto_merged['Borough'], df_toronto_merged['Neighbourhood']):
    label = '{}, {}'.format(neighborhood, borough)
    label = folium.Popup(label, parse_html=True)
    folium.CircleMarker(
        [lat, lng],
        radius=5,
        popup=label,
        color='blue',
        fill=True,
        fill_color='#3186cc',
        fill_opacity=0.7,
        parse_html=False).add_to(map_toronto)  
    
map_toronto
# add markers to map`


Make this Notebook Trusted to load map: File -> Trust Notebook

`West_toronto = df_toronto_merged[df_toronto_merged['Borough']=='West Toronto']
East_toronto = df_toronto_merged[df_toronto_merged['Borough']=='East Toronto']
Central_toronto = df_toronto_merged[df_toronto_merged['Borough']=='Central Toronto']
Downtown_toronto = df_toronto_merged[df_toronto_merged['Borough']=='Downtown Toronto']
Downtown_toronto`


`toronto_boroughs = pd.concat([West_toronto, East_toronto,Central_toronto, Downtown_toronto], ignore_index=True)
toronto_boroughs`


`CLIENT_ID = '3JINVH0JKPGOMREVKMYU2R0V3I0OIHNAC3IJCNNT1E4Y5DFY' # your Foursquare ID
CLIENT_SECRET = 'JDHY3DFWLR2V1TPGU3LL253IYUN3C1CK0COB0XD5UPVPUDD0' # your Foursquare Secret
VERSION = '20180605' # Foursquare API version
LIMIT = 100 # A default Foursquare API limit value

print('Your credentails:')
print('CLIENT_ID: ' + CLIENT_ID)
print('CLIENT_SECRET:' + CLIENT_SECRET)`


Get the latitude and longitude value of the first neighbourhood in the data frame:

`neighborhood_latitude = toronto_boroughs.loc[0, 'Latitude'] # neighborhood latitude value
neighborhood_longitude = toronto_boroughs.loc[0, 'Longitude'] # neighborhood longitude value

neighborhood_name = toronto_boroughs.loc[0, 'Neighbourhood'] # neighborhood name

print('Latitude and longitude values of {} are {}, {}.'.format(neighborhood_name, 
                                                               neighborhood_latitude, 
                                                               neighborhood_longitude))`


We get the top 50 venues around the neighbourhood in the following steps:

`# type your answer here
LIMIT = 50
radius = 500
url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
    CLIENT_ID, 
    CLIENT_SECRET, 
    VERSION, 
    neighborhood_latitude, 
    neighborhood_longitude, 
    radius, 
    LIMIT)
print(url)`

`results = requests.get(url).json()
results`




We use the function defined in the lab to get the categories of the venues, first we define it:

`# function that extracts the category of the venue
def get_category_type(row):
    try:
        categories_list = row['categories']
    except:
        categories_list = row['venue.categories']
        
    if len(categories_list) == 0:
        return None
    else:
        return categories_list[0]['name']`

We put the json into a dataframe

`venues = results['response']['groups'][0]['items']
    
nearby_venues = json_normalize(venues) # flatten JSON
print(nearby_venues)
# filter columns
filtered_columns = ['venue.name', 'venue.categories', 'venue.location.lat', 'venue.location.lng']
nearby_venues =nearby_venues.loc[:, filtered_columns]

# filter the category for each row
nearby_venues['venue.categories'] = nearby_venues.apply(get_category_type, axis=1)

# clean columns
nearby_venues.columns = [col.split(".")[-1] for col in nearby_venues.columns]

nearby_venues.head()`


`/opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages/ipykernel/__main__.py:3: FutureWarning: pandas.io.json.json_normalize is deprecated, use pandas.json_normalize instead
  app.launch_new_instance()`


We check how many venues were returned

`print('{} venues were returned by Foursquare.'.format(nearby_venues.shape[0]))`


**Explore Toronto Neighbourhoods**

This gives us an idea how the single steps need to be undertaken in order to get the venues for one neighbourhood with Foursquare. Now, we repeat the same process for all toronto neighbourhoods which we have in the toronto_boroughs data frame. First, we define a function which helps us to do this:

`def getNearbyVenues(names, latitudes, longitudes, radius=500):
    
    venues_list=[]
    for name, lat, lng in zip(names, latitudes, longitudes):
        print(name)
            
        # create the API request URL
        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            radius, 
            LIMIT)
            
        # make the GET request
        results = requests.get(url).json()["response"]['groups'][0]['items']
        
        # return only relevant information for each nearby venue
        venues_list.append([(
            name, 
            lat, 
            lng, 
            v['venue']['name'], 
            v['venue']['location']['lat'], 
            v['venue']['location']['lng'],  
            v['venue']['categories'][0]['name']) for v in results])

    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])
    nearby_venues.columns = ['Neighborhood', 
                  'Neighborhood Latitude', 
                  'Neighborhood Longitude', 
                  'Venue', 
                  'Venue Latitude', 
                  'Venue Longitude', 
                  'Venue Category']
    
    return(nearby_venues)`

We execute the function with out toronto_neighbours data frame

`toronto_venues = getNearbyVenues(names = toronto_boroughs['Neighbourhood'], latitudes = toronto_boroughs['Latitude'], longitudes = toronto_boroughs['Longitude'])`

`print(toronto_venues.shape)
toronto_venues.head(40)`

`toronto_venues.groupby('Neighborhood').count()`

Checking how many unique categories there are for the venues:

`print('There are {} uniques categories.'.format(len(toronto_venues['Venue Category'].unique())))`


**Analyze each Neighbourhood**

For the subsequent analysis we use one hot encoding on the toronto_venues data frame

`# one hot encoding
toronto_onehot = pd.get_dummies(toronto_venues[['Venue Category']], prefix="", prefix_sep="")

# add neighborhood column back to dataframe
toronto_onehot['Neighborhood'] = toronto_venues['Neighborhood'] 

# move neighborhood column to the first column
fixed_columns = [toronto_onehot.columns[-1]] + list(toronto_onehot.columns[:-1])
toronto_onehot = toronto_onehot[fixed_columns]

toronto_onehot.head()`

`toronto_onehot.shape`



We would like to know the frequency of occurence of a category for each of the neighbourhoods, thus:

toronto_grouped = toronto_onehot.groupby('Neighborhood').mean().reset_index() toronto_grouped.head()


`toronto_grouped.shape`

We print the neighbourhood with the top 5 most popular venues

`num_top_venues = 5

for hood in toronto_grouped['Neighborhood']:
    print("----"+hood+"----")
    temp = toronto_grouped[toronto_grouped['Neighborhood'] == hood].T.reset_index()
    temp.columns = ['venue','freq']
    temp = temp.iloc[1:]
    temp['freq'] = temp['freq'].astype(float)
    temp = temp.round({'freq': 2})
    print(temp.sort_values('freq', ascending=False).reset_index(drop=True).head(num_top_venues))
    print('\n')`

`def return_most_common_venues(row, num_top_venues):
    row_categories = row.iloc[1:]
    row_categories_sorted = row_categories.sort_values(ascending=False)
    
    return row_categories_sorted.index.values[0:num_top_venues]`

`num_top_venues = 5

indicators = ['st', 'nd', 'rd']

# create columns according to number of top venues
columns = ['Neighborhood']
for ind in np.arange(num_top_venues):
    try:
        columns.append('{}{} Most Common Venue'.format(ind+1, indicators[ind]))
    except:
        columns.append('{}th Most Common Venue'.format(ind+1))

# create a new dataframe
neighborhoods_venues_sorted = pd.DataFrame(columns=columns)
neighborhoods_venues_sorted['Neighborhood'] = toronto_grouped['Neighborhood']

for ind in np.arange(toronto_grouped.shape[0]):
    neighborhoods_venues_sorted.iloc[ind, 1:] = return_most_common_venues(toronto_grouped.iloc[ind, :], num_top_venues)

neighborhoods_venues_sorted.head()`



Cluster Neighbourhood

The toronto neighbourhood is clustered into 3 clusters via applying the KMeans constructor that produces the labels and clusters the data

`# set number of clusters
kclusters = 3

toronto_grouped_clustering = toronto_grouped.drop('Neighborhood', 1)

# run k-means clustering
kmeans = KMeans(n_clusters=kclusters, random_state=0).fit(toronto_grouped_clustering)

# check cluster labels generated for each row in the dataframe
kmeans.labels_[0:50]`



`toronto_boroughs.rename(columns={"Neighbourhood": "Neighborhood"}, inplace=True)
toronto_boroughs`
